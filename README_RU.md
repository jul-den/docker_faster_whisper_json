# Универсальный JSON-ориентированный обработчик видео/аудио на базе Faster Whisper

*English version is available in [`README.md`](README.md).*

Данный инструмент позволяет автоматизировать расшифровку вебинаров, лекций и других видео и аудио записей с последующей обработкой текста с помощью LLM (например, через LM Studio). Всё управление происходит через JSON-файлы, которые описывают последовательность шагов: склейка фрагментов, извлечение аудио, нарезка по времени, транскрибация Faster Whisper (далее Whisper) и отправка запросов к LLM.

## Возможности

- **Пакетная обработка** – можно положить несколько JSON-задач в папку `/source`, скрипт выполнит их по очереди.
- **Гибкое описание шагов** – поддерживаются операции:
  - `concat` – склейка видеофайлов (например, из нескольких `.ts` в один `.mp4`).
  - `extract_audio` – извлечение аудиодорожки из видео в WAV (16 кГц, моно).
  - `segment` – нарезка аудио на сегменты по временны́м меткам.
  - `transcribe` – распознавание речи (Whisper) с поддержкой масок для нескольких файлов.
  - `llm` – отправка текста в LLM (LM Studio) с заданным промптом.
- **Ленивая загрузка модели Whisper** – модель загружается только при первом вызове `transcribe_audio`.
- **Пропуск уже выполненных шагов** – если выходной файл существует, шаг не выполняется повторно.
- **Офлайн-режим** – после однократной загрузки модели все операции идут без обращений в интернет.
- **Переопределение параметров** – в каждом JSON можно задать свои настройки (модель Whisper, модель LLM, URL и т.д.), которые имеют приоритет над переменными окружения.

## Требования

- Установленный **Docker** (желательно с поддержкой монтирования томов).
- **LM Studio** (опционально, если использовать шаги локальной `llm`). Сервер должен быть запущен и доступен из контейнера (обычно по адресу `http://host.docker.internal:1234` для Windows/macOS).
- **Модели Whisper** должны быть предварительно загружены в кэш (обычно `~/.cache/huggingface/hub/`). При первом запуске без кэша будет попытка скачать модель – это нормально, но для офлайн-режима после первой загрузки модель останется в кэше.

## Структура JSON-задачи

Каждый JSON-файл должен содержать следующие поля:

- `type` (обязательно) – должно быть `"video_transcription"` (скрипт игнорирует файлы с другим типом).
- `name` (опционально) – имя задачи; используется для создания подпапки в `/output`. Если не указано, берётся имя файла (без расширения).
- `settings` (опционально) – объект с параметрами, переопределяющими глобальные переменные:
  - `whisper_model` – модель Whisper (например, `"large-v3"`, `"medium"`).
  - `lm_model` – модель, используемая в LM Studio (например, `"qwen/qwen3-8b"`).
  - `lm_url` – URL эндпоинта LM Studio (по умолчанию `http://host.docker.internal:1234/v1/chat/completions`).
  - `lm_token` - Токен для доступа к API (по умолчанию нет)
  - `delete_wav` – удалять ли временные `.wav` файлы после транскрибации (`true`/`false`).
- `steps` – массив шагов. Каждый шаг содержит поле `action` и дополнительные параметры, зависящие от действия.

### Параметры шагов

#### `concat` – склейка файлов (любых файлов, совместимых с concat demuxer'ом FFmpeg)
- `input` – маска файлов (например, `"media_*.ts"`).
- `output` – имя результирующего mp4 файла.
- `type` – `"video"` (поддерживается только видео).

#### `extract_audio` – извлечение аудио
- `input` – имя входного видеофайла.
- `output` – имя выходного WAV-файла.
- `params` – объект с параметрами ffmpeg (по умолчанию `{"ar": 16000, "ac": 1}`).

#### `segment` – нарезка аудио на сегменты
- `input` – имя исходного аудиофайла.
- `output_prefix` – префикс для имён сегментов (к ним добавится номер и расширение).
- `segments` – массив объектов с полями `start` и `end` (в секундах).
- `format` – расширение выходных файлов (по умолчанию `"wav"`).

#### `transcribe` – транскрибация аудио
- `input` – маска аудиофайлов (например, `"part2_doklad_seg*.wav"`). Файлы ищутся сначала в папке задачи, затем в `/source`.
- `output` – имя текстового файла, куда будет записан результат.
- `language` – язык (по умолчанию `"ru"`).
- `beam_size` – параметр для Whisper (по умолчанию `5`).
- `combine` – если `true` (по умолчанию), тексты всех найденных файлов объединяются через символ новой строки. Если `false`, берётся только первый по алфавиту файл.

#### `llm` – запрос к LM Studio
- `input` – имя текстового файла с исходным текстом.
- `output` – имя файла для ответа LLM.
- `prompt` – текст промпта. Нужно использовать плейсхолдер `{input_text}`, который заменится на содержимое входного файла.
- `system` – опциональный системный промпт.
- `max_tokens` – максимальное количество токенов в ответе (по умолчанию `8000`).

### Пример JSON для вебинара (несколько шагов)

```json
{
  "type": "video_transcription",
  "name": "webinar",
  "settings": {
    "delete_wav": true,
    "whisper_model": "large-v3",
    "lm_model": "qwen/qwen3-8b"
  },
  "steps": [
    {
      "action": "concat",
      "input": "media_*.ts",
      "output": "full_video.mp4",
      "type": "video"
    },
    {
      "action": "extract_audio",
      "input": "full_video.mp4",
      "output": "full_audio.wav"
    },
    {
      "action": "segment",
      "input": "full_audio.wav",
      "output_prefix": "part2_doklad_seg",
      "segments": [
        {"start": 252.0, "end": 1440.0}
      ]
    },
    {
      "action": "transcribe",
      "input": "part2_doklad_seg*.wav",
      "output": "part2_doklad.txt",
      "language": "ru"
    },
    {
      "action": "llm",
      "input": "part2_doklad.txt",
      "output": "summary_doklad.txt",
      "prompt": "Сделай краткое содержание доклада по следующей расшифровке:\n\n{input_text}"
    },
    {
      "action": "segment",
      "input": "full_audio.wav",
      "output_prefix": "part4_qa_seg",
      "segments": [
        {"start": 1485.0, "end": 4419.0}
      ]
    },
    {
      "action": "transcribe",
      "input": "part4_qa_seg*.wav",
      "output": "part4_qa.txt",
      "language": "ru"
    },
    {
      "action": "llm",
      "input": "part4_qa.txt",
      "output": "qa_pairs.txt",
      "prompt": "Ниже представлена расшифровка сессии вопросов и ответов вебинара. Извлеки из неё все пары \"Вопрос\" и \"Ответ\". Игнорируй слова ведущей типа \"Принимаем вопрос\", \"Следующий вопрос\" и технические комментарии. Выведи каждую пару в формате:\n\nВопрос: <текст вопроса>\nОтвет: <текст ответа>\n---\n(разделитель между парами)\n\nТекст расшифровки:\n{input_text}",
      "max_tokens": 8000
    },
    {
      "action": "transcribe",
      "input": "full_audio.wav",
      "output": "full_transcript.txt",
      "language": "ru"
    }
  ]
}
```

## Запуск Docker-контейнера

1. **Соберите образ** (если ещё не сделали):
   ```bash
   docker build -t webinar-processor .
   ```

2. **Подготовьте папки**:
   - Создайте на ПК папку для исходных файлов. Положите туда все нужные файлы (например, `.ts`) и JSON-задачу.
   - Создайте папку на ПК для результатов.

3. **Запустите контейнер** (пример для Windows PowerShell):
   ```powershell
   docker run -d `
     --name webinar-processor-container `
     -v C:\PATH\TO\source:/source `
     -v C:\PATH\TO\output:/app/output `
     -e VIDEO_DIR=/source `
     -e OUTPUT_DIR=/app/output `
     -e LLM_URL="http://host.docker.internal:1234/v1/chat/completions" `
     -e LLM_API_TOKEN="TOKEN" `
     webinar-processor
   ```
   Если используете Linux, замените `host.docker.internal` на `localhost` и добавьте флаг `--network="host"`.

4. **Просмотр логов**:
   ```bash
   docker logs -f webinar-processor-container
   ```

5. **Остановка и удаление**:
   ```bash
   docker stop webinar-processor-container
   docker rm webinar-processor-container
   ```

## Переменные окружения (можно задать при запуске, но не обязательны)

- `VIDEO_DIR` – папка с исходными файлами и JSON (по умолчанию `/source`).
- `OUTPUT_DIR` – папка для результатов (по умолчанию `/app/output`).
- `LLM_URL` – URL эндпоинта LLM (LM Studio, по умолчанию `http://host.docker.internal:1234/v1/chat/completions`). Можна указать эндпоинт любой совместимой LLM (подобные OpenAI).
- `LLM_API_TOKEN` - Токен для доступа к API (по умолчанию не используется)
- `LLM_MODEL` – модель по умолчанию для LLM-шагов (по умолчанию `qwen/qwen3-8b`).
- `WHISPER_MODEL` – модель Whisper (по умолчанию `large-v3`).
- `DELETE_WAV_AFTER_TRANSCRIBE` – удалять ли временные WAV после транскрибации (`true`/`false`). По умолчанию `true`.

## Примечания

- Модель Whisper загружается **лениво** – только при первом вызове `transcribe_audio`. Если все тексты уже расшифрованы (файлы `*.txt` существуют), модель вообще не будет загружена.
- Для офлайн-режима рекомендуется предварительно загрузить нужную модель Whisper (например, через отдельный запуск с доступом в интернет). После этого можно установить переменные `HF_HUB_OFFLINE=1` и `TRANSFORMERS_OFFLINE=1` (они уже прописаны в `get_whisper_model`).
- Все пути в JSON интерпретируются как относительные:
  - для входных файлов сначала ищем в `/output/<имя_задачи>/`, затем в `/source/`;
  - для выходных файлов всегда используем `/output/<имя_задачи>/`.
- Если в шаге `transcribe` указана маска, и `combine: true`, все найденные тексты объединяются. Промежуточные тексты каждого сегмента сохраняются в файлы с тем же именем, но с расширением `.txt` (рядом с WAV).
- При возникновении ошибки в любом шаге выполнение текущей задачи прерывается (переход к следующему JSON).


## Поддержка

Если возникнут вопросы или предложения, создавайте issue или обращайтесь к автору.
